{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os, shutil\n",
    "\n",
    "from MusicRnnData import MusicRnnData\n",
    "\n",
    "# input parameters\n",
    "x_len = 10\n",
    "y_len = 1\n",
    "batch_size = 32\n",
    "# LSTM parameters\n",
    "num_layers = 1\n",
    "lstm_size = 8\n",
    "hidden_size = 8\n",
    "# training parameters\n",
    "dropout_prob = 0.5\n",
    "learning_rate = 1e-1\n",
    "num_steps = 50000\n",
    "verbose = True\n",
    "display_interval = 500\n",
    "moving_avg_length = 100\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class MovingAverager(object):\n",
    "    def __init__(self, filter_length):\n",
    "        self.filter = deque([0 for _ in range(filter_length)])\n",
    "        \n",
    "    def insert(self, num):\n",
    "        self.filter.popleft()\n",
    "        self.filter.append(num)\n",
    "        \n",
    "    def average(self):\n",
    "        return sum(self.filter)/float(len(self.filter))\n",
    "    \n",
    "def build_lstm_stack(num_layers, lstm_size, dropout_prob=1.0):\n",
    "    def lstm_layer(lstm_size, dropout_prob=1.0):\n",
    "        lstm_layer = tf.contrib.rnn.BasicLSTMCell(lstm_size, forget_bias=0.0)\n",
    "        return tf.contrib.rnn.DropoutWrapper(lstm_layer, output_keep_prob=dropout_prob)\n",
    "    \n",
    "    lstm_stack = [lstm_layer(lstm_size, dropout_prob=dropout_prob) for _ in range(num_layers)]\n",
    "    return tf.contrib.rnn.MultiRNNCell(lstm_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filelist = ['a2002011001-e02.wav']\n",
    "filelist = ['sine.wav']\n",
    "music_data = MusicRnnData(filelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input\n",
    "x = tf.placeholder(tf.float32, [batch_size, x_len])\n",
    "y = tf.placeholder(tf.float32, [batch_size, y_len])\n",
    "\n",
    "# RNN Cell\n",
    "lstm = build_lstm_stack(num_layers, lstm_size, dropout_prob=dropout_prob)\n",
    "# add dropout here\n",
    "\n",
    "# output layer\n",
    "output, states = tf.contrib.rnn.static_rnn(lstm, [x], dtype=tf.float32)\n",
    "fc_weights = tf.Variable(tf.random_normal([hidden_size, y_len]))\n",
    "fc_bias = tf.Variable(tf.random_normal([y_len]))\n",
    "\n",
    "y_ = tf.matmul(output[-1], fc_weights) + fc_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set cost function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cost = tf.divide(tf.nn.l2_loss(y_ - y), y_len)\n",
    "cost = tf.reduce_mean(tf.squared_difference(y_, y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set summary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir_save = './Logs'\n",
    "\n",
    "# define summary for tensorboard\n",
    "tf.summary.scalar('loss', cost)\n",
    "tf.summary.histogram('fc_weights', fc_weights)\n",
    "tf.summary.histogram('fc_bias', fc_bias)\n",
    "tf.summary.histogram('lstm_output', output)\n",
    "tf.summary.histogram('prediction', y_)\n",
    "summary_merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, training loss: 0.176657\n",
      "step: 500, training loss: 0.00144034\n",
      "moving average loss: 0.00494849\n",
      "step: 1000, training loss: 0.00134713\n",
      "moving average loss: 0.00485505\n",
      "step: 1500, training loss: 0.00494837\n",
      "moving average loss: 0.00422645\n",
      "step: 2000, training loss: 0.00782667\n",
      "moving average loss: 0.00469519\n",
      "step: 2500, training loss: 0.0039129\n",
      "moving average loss: 0.0045455\n",
      "step: 3000, training loss: 0.00519761\n",
      "moving average loss: 0.00430846\n",
      "step: 3500, training loss: 0.000193236\n",
      "moving average loss: 0.00414276\n",
      "step: 4000, training loss: 0.00527414\n",
      "moving average loss: 0.00442357\n",
      "step: 4500, training loss: 0.01435\n",
      "moving average loss: 0.00395536\n",
      "step: 5000, training loss: 0.00221595\n",
      "moving average loss: 0.00389955\n",
      "step: 5500, training loss: 0.00169103\n",
      "moving average loss: 0.00457257\n",
      "step: 6000, training loss: 0.00474518\n",
      "moving average loss: 0.00445166\n",
      "step: 6500, training loss: 0.00187249\n",
      "moving average loss: 0.0040916\n",
      "step: 7000, training loss: 0.00316414\n",
      "moving average loss: 0.00438493\n",
      "step: 7500, training loss: 0.00678697\n",
      "moving average loss: 0.00429829\n",
      "step: 8000, training loss: 0.000591447\n",
      "moving average loss: 0.00551301\n",
      "step: 8500, training loss: 0.000980786\n",
      "moving average loss: 0.00413269\n",
      "step: 9000, training loss: 0.00109129\n",
      "moving average loss: 0.00394313\n",
      "step: 9500, training loss: 0.00212658\n",
      "moving average loss: 0.00359747\n",
      "step: 10000, training loss: 0.00233929\n",
      "moving average loss: 0.00424978\n",
      "step: 10500, training loss: 0.00386038\n",
      "moving average loss: 0.0044931\n",
      "step: 11000, training loss: 0.00225995\n",
      "moving average loss: 0.00463914\n",
      "step: 11500, training loss: 0.00200384\n",
      "moving average loss: 0.00427742\n",
      "step: 12000, training loss: 0.00350088\n",
      "moving average loss: 0.00542272\n",
      "step: 12500, training loss: 0.00414011\n",
      "moving average loss: 0.00423173\n",
      "step: 13000, training loss: 0.000676269\n",
      "moving average loss: 0.004244\n",
      "step: 13500, training loss: 0.00945034\n",
      "moving average loss: 0.00463107\n",
      "step: 14000, training loss: 0.0033149\n",
      "moving average loss: 0.00450933\n",
      "step: 14500, training loss: 0.00106369\n",
      "moving average loss: 0.00506346\n",
      "step: 15000, training loss: 0.00199769\n",
      "moving average loss: 0.00448899\n",
      "step: 15500, training loss: 0.00149992\n",
      "moving average loss: 0.00405513\n",
      "step: 16000, training loss: 0.00479968\n",
      "moving average loss: 0.00421945\n",
      "step: 16500, training loss: 0.00205087\n",
      "moving average loss: 0.00398776\n",
      "step: 17000, training loss: 0.000225336\n",
      "moving average loss: 0.00386018\n",
      "step: 17500, training loss: 0.0022236\n",
      "moving average loss: 0.00447507\n",
      "step: 18000, training loss: 0.00708523\n",
      "moving average loss: 0.00390937\n",
      "step: 18500, training loss: 0.000791922\n",
      "moving average loss: 0.00394966\n",
      "step: 19000, training loss: 0.00248929\n",
      "moving average loss: 0.00385116\n",
      "step: 19500, training loss: 0.000904192\n",
      "moving average loss: 0.0046535\n",
      "step: 20000, training loss: 0.00297517\n",
      "moving average loss: 0.0046536\n",
      "step: 20500, training loss: 0.00250813\n",
      "moving average loss: 0.004134\n",
      "step: 21000, training loss: 0.000690524\n",
      "moving average loss: 0.00405011\n",
      "step: 21500, training loss: 0.00151856\n",
      "moving average loss: 0.00454566\n",
      "step: 22000, training loss: 0.0303195\n",
      "moving average loss: 0.00388139\n",
      "step: 22500, training loss: 0.00537359\n",
      "moving average loss: 0.00754802\n",
      "step: 23000, training loss: 0.109444\n",
      "moving average loss: 0.00558291\n",
      "step: 23500, training loss: 0.00967627\n",
      "moving average loss: 0.00447647\n",
      "step: 24000, training loss: 0.00327363\n",
      "moving average loss: 0.00402705\n",
      "step: 24500, training loss: 0.00540546\n",
      "moving average loss: 0.00450745\n",
      "step: 25000, training loss: 0.0145044\n",
      "moving average loss: 0.00493965\n",
      "step: 25500, training loss: 0.00421361\n",
      "moving average loss: 0.0048323\n",
      "step: 26000, training loss: 0.00944679\n",
      "moving average loss: 0.00439231\n",
      "step: 26500, training loss: 0.00739292\n",
      "moving average loss: 0.00484069\n",
      "step: 27000, training loss: 0.00207645\n",
      "moving average loss: 0.00464449\n",
      "step: 27500, training loss: 0.000819399\n",
      "moving average loss: 0.00532286\n",
      "step: 28000, training loss: 0.0232136\n",
      "moving average loss: 0.00524188\n",
      "step: 28500, training loss: 0.000767084\n",
      "moving average loss: 0.00416068\n",
      "step: 29000, training loss: 0.000505014\n",
      "moving average loss: 0.00359059\n",
      "step: 29500, training loss: 0.00705412\n",
      "moving average loss: 0.00435363\n",
      "step: 30000, training loss: 0.00805159\n",
      "moving average loss: 0.00429664\n",
      "step: 30500, training loss: 0.00564664\n",
      "moving average loss: 0.0046093\n",
      "step: 31000, training loss: 0.00143167\n",
      "moving average loss: 0.0038512\n",
      "step: 31500, training loss: 0.00371745\n",
      "moving average loss: 0.00416184\n",
      "step: 32000, training loss: 0.00357515\n",
      "moving average loss: 0.00476497\n",
      "step: 32500, training loss: 0.0314609\n",
      "moving average loss: 0.00472623\n",
      "step: 33000, training loss: 0.00219766\n",
      "moving average loss: 0.00389449\n",
      "step: 33500, training loss: 0.00269921\n",
      "moving average loss: 0.00448269\n",
      "step: 34000, training loss: 0.00458064\n",
      "moving average loss: 0.00475577\n",
      "step: 34500, training loss: 0.000862005\n",
      "moving average loss: 0.00372304\n",
      "step: 35000, training loss: 0.00141742\n",
      "moving average loss: 0.00399148\n",
      "step: 35500, training loss: 0.00339249\n",
      "moving average loss: 0.00395917\n",
      "step: 36000, training loss: 0.00234754\n",
      "moving average loss: 0.00419258\n",
      "step: 36500, training loss: 0.00811461\n",
      "moving average loss: 0.00453005\n",
      "step: 37000, training loss: 0.00570458\n",
      "moving average loss: 0.00389877\n",
      "step: 37500, training loss: 0.00519942\n",
      "moving average loss: 0.00458676\n",
      "step: 38000, training loss: 2.03667e-06\n",
      "moving average loss: 0.00454798\n",
      "step: 38500, training loss: 0.00299352\n",
      "moving average loss: 0.00413509\n",
      "step: 39000, training loss: 0.00202464\n",
      "moving average loss: 0.00431501\n",
      "step: 39500, training loss: 0.0104539\n",
      "moving average loss: 0.00425152\n",
      "step: 40000, training loss: 0.0118691\n",
      "moving average loss: 0.0048816\n",
      "step: 40500, training loss: 0.00437027\n",
      "moving average loss: 0.00543142\n",
      "step: 41000, training loss: 0.00526457\n",
      "moving average loss: 0.00411064\n",
      "step: 41500, training loss: 0.000173913\n",
      "moving average loss: 0.00650393\n",
      "step: 42000, training loss: 0.00807732\n",
      "moving average loss: 0.00413598\n",
      "step: 42500, training loss: 0.00275976\n",
      "moving average loss: 0.00376939\n",
      "step: 43000, training loss: 0.000618108\n",
      "moving average loss: 0.004339\n",
      "step: 43500, training loss: 0.00295847\n",
      "moving average loss: 0.00433285\n",
      "step: 44000, training loss: 1.95979e-05\n",
      "moving average loss: 0.0039757\n",
      "step: 44500, training loss: 0.00012102\n",
      "moving average loss: 0.00554112\n",
      "step: 45000, training loss: 0.000596983\n",
      "moving average loss: 0.00599104\n",
      "step: 45500, training loss: 0.000980631\n",
      "moving average loss: 0.00457662\n",
      "step: 46000, training loss: 0.00251513\n",
      "moving average loss: 0.00369375\n",
      "step: 46500, training loss: 2.41416e-05\n",
      "moving average loss: 0.00445347\n",
      "step: 47000, training loss: 0.000429973\n",
      "moving average loss: 0.00600002\n",
      "step: 47500, training loss: 0.00640667\n",
      "moving average loss: 0.00438918\n",
      "step: 48000, training loss: 0.000560959\n",
      "moving average loss: 0.00382742\n",
      "step: 48500, training loss: 0.00486185\n",
      "moving average loss: 0.00426104\n",
      "step: 49000, training loss: 0.00218713\n",
      "moving average loss: 0.00413771\n",
      "step: 49500, training loss: 0.00265654\n",
      "moving average loss: 0.00386407\n"
     ]
    }
   ],
   "source": [
    "# initialize moving averager for loss\n",
    "if verbose:\n",
    "    moving_avg = MovingAverager(moving_avg_length)\n",
    "\n",
    "# create session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# define saver\n",
    "if os.path.exists(dir_save):\n",
    "    shutil.rmtree(dir_save)\n",
    "train_writer = tf.summary.FileWriter(dir_save, sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# and begin\n",
    "for i in range(num_steps):\n",
    "    x_batch, y_batch = music_data.batch(x_len, y_len, batch_size)\n",
    "    _, loss, summary = sess.run([optimizer, cost, summary_merged], feed_dict={x: x_batch, y: y_batch})\n",
    "    \n",
    "    train_writer.add_summary(summary, i)\n",
    "\n",
    "    if verbose:\n",
    "        moving_avg.insert(loss)\n",
    "        # print progress\n",
    "        if (i % display_interval == 0):\n",
    "            print('step: %d, training loss: %g' % (i, loss))\n",
    "\n",
    "            if i > moving_avg_length:\n",
    "                print('moving average loss: %g' % (moving_avg.average()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, len(prediction) = 11\n",
      "Iteration: 1000, len(prediction) = 1011\n",
      "Iteration: 2000, len(prediction) = 2011\n",
      "Iteration: 3000, len(prediction) = 3011\n",
      "Iteration: 4000, len(prediction) = 4011\n",
      "Iteration: 5000, len(prediction) = 5011\n",
      "Iteration: 6000, len(prediction) = 6011\n",
      "Iteration: 7000, len(prediction) = 7011\n",
      "Iteration: 8000, len(prediction) = 8011\n",
      "Iteration: 9000, len(prediction) = 9011\n",
      "Iteration: 10000, len(prediction) = 10011\n",
      "Iteration: 11000, len(prediction) = 11011\n",
      "Iteration: 12000, len(prediction) = 12011\n",
      "Iteration: 13000, len(prediction) = 13011\n",
      "Iteration: 14000, len(prediction) = 14011\n",
      "Iteration: 15000, len(prediction) = 15011\n",
      "Iteration: 16000, len(prediction) = 16011\n",
      "Iteration: 17000, len(prediction) = 17011\n",
      "Iteration: 18000, len(prediction) = 18011\n",
      "Iteration: 19000, len(prediction) = 19011\n",
      "Iteration: 20000, len(prediction) = 20011\n",
      "Iteration: 21000, len(prediction) = 21011\n",
      "Iteration: 22000, len(prediction) = 22011\n",
      "Iteration: 23000, len(prediction) = 23011\n",
      "Iteration: 24000, len(prediction) = 24011\n",
      "Iteration: 25000, len(prediction) = 25011\n",
      "Iteration: 26000, len(prediction) = 26011\n",
      "Iteration: 27000, len(prediction) = 27011\n",
      "Iteration: 28000, len(prediction) = 28011\n",
      "Iteration: 29000, len(prediction) = 29011\n",
      "Iteration: 30000, len(prediction) = 30011\n",
      "Iteration: 31000, len(prediction) = 31011\n",
      "Iteration: 32000, len(prediction) = 32011\n",
      "Iteration: 33000, len(prediction) = 33011\n",
      "Iteration: 34000, len(prediction) = 34011\n",
      "Iteration: 35000, len(prediction) = 35011\n",
      "Iteration: 36000, len(prediction) = 36011\n",
      "Iteration: 37000, len(prediction) = 37011\n",
      "Iteration: 38000, len(prediction) = 38011\n",
      "Iteration: 39000, len(prediction) = 39011\n",
      "Iteration: 40000, len(prediction) = 40011\n",
      "Iteration: 41000, len(prediction) = 41011\n",
      "Iteration: 42000, len(prediction) = 42011\n",
      "Iteration: 43000, len(prediction) = 43011\n",
      "Iteration: 44000, len(prediction) = 44011\n",
      "Iteration: 45000, len(prediction) = 45011\n",
      "Iteration: 46000, len(prediction) = 46011\n",
      "Iteration: 47000, len(prediction) = 47011\n",
      "Iteration: 48000, len(prediction) = 48011\n",
      "Iteration: 49000, len(prediction) = 49011\n",
      "Iteration: 50000, len(prediction) = 50011\n",
      "Iteration: 51000, len(prediction) = 51011\n",
      "Iteration: 52000, len(prediction) = 52011\n",
      "Iteration: 53000, len(prediction) = 53011\n",
      "Iteration: 54000, len(prediction) = 54011\n",
      "Iteration: 55000, len(prediction) = 55011\n",
      "Iteration: 56000, len(prediction) = 56011\n",
      "Iteration: 57000, len(prediction) = 57011\n",
      "Iteration: 58000, len(prediction) = 58011\n",
      "Iteration: 59000, len(prediction) = 59011\n",
      "Iteration: 60000, len(prediction) = 60011\n",
      "Iteration: 61000, len(prediction) = 61011\n",
      "Iteration: 62000, len(prediction) = 62011\n",
      "Iteration: 63000, len(prediction) = 63011\n",
      "Iteration: 64000, len(prediction) = 64011\n",
      "Iteration: 65000, len(prediction) = 65011\n",
      "Iteration: 66000, len(prediction) = 66011\n",
      "Iteration: 67000, len(prediction) = 67011\n",
      "Iteration: 68000, len(prediction) = 68011\n",
      "Iteration: 69000, len(prediction) = 69011\n",
      "Iteration: 70000, len(prediction) = 70011\n",
      "Iteration: 71000, len(prediction) = 71011\n",
      "Iteration: 72000, len(prediction) = 72011\n",
      "Iteration: 73000, len(prediction) = 73011\n",
      "Iteration: 74000, len(prediction) = 74011\n",
      "Iteration: 75000, len(prediction) = 75011\n",
      "Iteration: 76000, len(prediction) = 76011\n",
      "Iteration: 77000, len(prediction) = 77011\n",
      "Iteration: 78000, len(prediction) = 78011\n",
      "Iteration: 79000, len(prediction) = 79011\n",
      "Iteration: 80000, len(prediction) = 80011\n",
      "Iteration: 81000, len(prediction) = 81011\n",
      "Iteration: 82000, len(prediction) = 82011\n",
      "Iteration: 83000, len(prediction) = 83011\n",
      "Iteration: 84000, len(prediction) = 84011\n",
      "Iteration: 85000, len(prediction) = 85011\n",
      "Iteration: 86000, len(prediction) = 86011\n",
      "Iteration: 87000, len(prediction) = 87011\n",
      "Iteration: 88000, len(prediction) = 88011\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "display_interval = 1000\n",
    "\n",
    "original = music_data.convert_to_wav(music_data.tracks[0])\n",
    "orig_len = original.shape[0]\n",
    "\n",
    "prediction = original[0:x_len]\n",
    "num_predictions = int((orig_len-x_len)/y_len)\n",
    "x_batch = prediction\n",
    "for i in range(num_predictions):\n",
    "    feed_pred = np.expand_dims(x_batch, axis=0)\n",
    "    feed_pred = np.repeat(feed_pred, batch_size, axis=0)\n",
    "    new_y = sess.run([y_], feed_dict={x: feed_pred, y: y_batch})[0][0,:]\n",
    "    prediction = np.append(prediction, new_y, axis=0)\n",
    "    x_batch = np.append(x_batch[y_len:], new_y, axis=0)\n",
    "    \n",
    "    if (i % display_interval == 0):\n",
    "        print('Iteration: %d, len(prediction) = %g' % (i, len(prediction)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot predicted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "converted = music_data.convert_to_wav(prediction)\n",
    "pred_len = converted.shape[0]\n",
    "x_orig = np.linspace(0, orig_len/music_data.sample_rate, orig_len)\n",
    "x_conv = np.linspace(0, pred_len/music_data.sample_rate, pred_len)\n",
    "plt.subplot(211)\n",
    "plt.plot(x_orig[0:300], original[0:300])\n",
    "plt.subplot(212)\n",
    "plt.plot(x_conv[0:300], converted[0:300])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
